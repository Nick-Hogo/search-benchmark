---
name: search-benchmark
description: 搜索能力基准测试 - 基于问答式评估的综合测试框架
version: 2.0.0
invocable: true
---

# 搜索能力基准测试 v2.0

这个 skill 用于测试搜索工具的综合能力，包括时效性、准确度、抗幻觉、可核查性和效率。

## 核心理念

> **测试的不是"返回了什么数据"，而是"能否正确回答问题"**

基于 document.md 的测试方法论：
- 每个测试问题都有明确的判定标准
- 避免主观评价，采用可验证的判点
- 测试"要么能查到、要么查不到、要么应该拒绝编造"的场景

## 测试架构

### 双层测试模型
1. **工具层测试**：评估搜索工具返回的原始数据质量
2. **模型层测试**：评估使用搜索工具后生成的答案质量

### 评分方式
使用 AI Judge（当前 Claude 实例）根据预定义的判定标准自动评分。

---

## 执行流程概览

当用户调用此 skill 时，按照以下四个阶段执行：

### 阶段一：初始化准备

**目标**：检查环境、加载配置、获取用户选择

**执行方式**：
```
Read: modules/01-setup.md
按照模块中的执行清单逐项完成所有初始化任务
```

**预期输出**：
- 测试会话目录已创建
- 用户已选择测试维度和工具
- 测试总数已确定

---

### 阶段二：测试执行循环

**⚠️ 重要执行原则**：
- **连续自动执行**：所有测试必须连续自动完成，不得在测试循环中暂停或等待用户确认
- **禁止中断询问**：完成一个测试后，立即自动开始下一个测试，无需询问用户是否继续
- **批量处理**：尽可能在单个响应中完成多个测试步骤（准备→执行→保存）
- **仅显示进度**：测试过程中只输出进度信息，不请求用户交互

**执行方式**：
```
对每个 (测试用例 × 工具) 组合：
  Read: modules/02-execution.md
  按照模块中的执行清单完成测试，并保存结果到答卷文件
```

**预期输出**：
- 所有测试用例已完成
- 答卷文件已生成：`{session_dir}/answers/{tool_id}.md`
- 进度信息已实时显示

---

### 阶段三：结果分析

**目标**：对每个工具的答卷进行质量分析

**执行方式**：
```
对每个工具的答卷文件：
  Read: {session_dir}/answers/{tool_id}.md
  Read: modules/03-judging.md
  按照模块中的分析流程生成分析报告
```

**预期输出**：
- 分析报告已生成：`{session_dir}/analysis/{tool_id}_analysis.md`

---

### 阶段四：报告生成

**目标**：生成汇总报告和对比报告

**执行方式**：
```
Read: modules/04-reporting.md
按照模块中的报告模板生成：
  - 单工具详细报告（每个工具一份）
  - 多工具对比报告（如果测试了多个工具）
```

**预期输出**：
- 报告文件已生成
- 向用户展示测试摘要和报告路径

---

## 完成标志

当以下所有条件满足时，skill 执行完成：
- ✅ 所有选中的测试用例已执行，答案已保存到答卷文件
- ✅ 所有答卷已完成分析，分析报告已生成
- ✅ 对比报告文件已生成
- ✅ 用户已收到结果摘要

---

## 实现原则

### 关键设计

1. **你是答题者，也是评判者**
   - 答题时：诚实使用搜索工具，给出最佳答案
   - 评判时：严格按判定标准，不偏袒任何工具

2. **答卷文件结构**
   - 每个工具生成一个答卷文件：`{session_dir}/answers/{tool_id}.md`
   - 答卷文件包含该工具的所有测试问题及答案
   - 每个问题包含三部分：
     * **原始搜索结果**：工具返回的完整 JSON 数据
     * **Claude 分析答案**：基于搜索结果生成的自然语言答案
     * **时间戳信息**：搜索开始/完成时间、答案生成完成时间
   - 分析时读取答卷文件，对每个问题进行分析
   - 分析结果保存到独立的分析报告：`{session_dir}/analysis/{tool_id}_analysis.md`

3. **严格遵循判定标准**
   - 不主观判断"答案好不好"
   - 只检查判定标准中列出的具体点
   - 参考评分校准示例：`modules/calibration.md`

4. **保持一致性**
   - 同一判定标准对所有工具采用相同尺度
   - 记录扣分理由，确保可追溯

5. **自动化执行策略**
   - **文件操作自动化**：所有答卷保存、分析报告追加、日志记录等文件写入操作均自动完成，无需暂停询问用户
   - **完整流程执行**：按标准流程完整执行所有测试，不使用任何加速或简化策略
   - **禁止循环中断**：在测试循环（阶段二）中，严禁在测试之间暂停或等待用户确认，必须连续自动执行完所有测试
   - **工具独立性**：仅使用测试框架自身的工具和数据，不依赖外部辅助工具
   - **进度透明**：每完成一个测试立即显示进度，让用户了解执行状态
   - **结果明确**：测试完成后清晰展示报告保存路径

### 错误处理
```
如果搜索工具调用失败:
  - 记录错误信息
  - 在报告中标注失败
  - 继续其他测试
```

---

## 模块索引

当需要详细实现时，按需读取以下模块：

| 模块文件 | 用途 | 何时读取 |
|---------|------|---------|
| `modules/01-setup.md` | 环境检查和初始化详细清单 | 阶段一 |
| `modules/02-execution.md` | 单个测试执行详细流程 | 阶段二 - 执行每个测试 |
| `modules/03-judging.md` | 结果分析详细流程 | 阶段三 - 分析答卷 |
| `modules/04-reporting.md` | 报告生成详细步骤和模板 | 阶段四 - 生成报告 |
| `modules/calibration.md` | 评分校准示例 | 分析阶段参考 |

---

## 答卷文件格式规范

每个工具生成一个答卷文件，包含该工具的所有测试问题及答案。

**文件路径**: `{session_dir}/answers/{tool_id}.md`

### 答卷文件结构

```markdown
# {Tool Name} 测试答卷

**生成时间**: {timestamp}
**工具标识**: {tool_id}
**测试用例数**: {total_cases}

---

## 问题 1: [{test_case_id}] {test_case_name}

**分类**: {category}
**问题描述**:
```
{test_case.prompt}
```

### 原始搜索结果

**搜索查询**: {构建的查询字符串}
**结果数量**: {result_count}
**搜索开始时间**: {ISO8601}
**搜索完成时间**: {ISO8601}
**搜索耗时**: {X.XX}秒

<details>
<summary>完整 JSON 数据（点击展开）</summary>

\`\`\`json
{raw_search_results 的完整 JSON}
\`\`\`

</details>

### Claude 分析答案

**答案生成开始**: {ISO8601}
**答案生成完成**: {ISO8601}
**分析耗时**: {X.XX}秒

{生成的完整自然语言答案}

### 时间统计

| 阶段 | 耗时 |
|------|------|
| 搜索阶段 | {X.XX}秒 |
| 分析阶段 | {X.XX}秒 |
| **总耗时** | **{X.XX}秒** |

---

## 问题 2: [{test_case_id}] {test_case_name}

{重复上述格式}

---

... (所有测试问题)

---

## 汇总统计

- **总测试数**: {count}
- **总耗时**: {X.XX}秒
- **平均耗时**: {X.XX}秒/题
- **最快响应**: {X.XX}秒 ({test_case_id})
- **最慢响应**: {X.XX}秒 ({test_case_id})
```

### 文件组织结构

完整的测试会话目录结构：

```
{session_dir}/
├── answers/                       # 答卷目录
│   ├── grok-search.md            # Grok Search 的答卷
│   └── builtin-websearch.md      # Built-in WebSearch 的答卷
├── analysis/                      # 分析报告目录
│   ├── grok-search_analysis.md   # Grok Search 的分析报告
│   └── builtin-websearch_analysis.md
├── _test_log.jsonl               # 测试日志（索引文件）
├── summary_comparison.md         # 多工具对比报告
├── grok-search_report.md         # 单工具详细报告
└── builtin-websearch_report.md
```

---

## 快速开始

首次运行建议：
1. 选择"全部"维度进行完整测试
2. 同时测试两个工具以获得对比数据
3. 完成后查看 `summary_comparison.md` 了解整体表现
