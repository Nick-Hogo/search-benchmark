---
name: search-benchmark
description: 搜索能力基准测试 - 基于问答式评估的综合测试框架
version: 2.0.0
invocable: true
---

# 🔍 搜索能力基准测试 v2.0

这个 skill 用于测试搜索工具的综合能力，包括时效性、准确度、抗幻觉、可核查性和效率。

## 核心理念

> **测试的不是"返回了什么数据"，而是"能否正确回答问题"**

基于 document.md 的测试方法论：
- ✅ 每个测试问题都有明确的判定标准
- ✅ 避免主观评价，采用可验证的判点
- ✅ 测试"要么能查到、要么查不到、要么应该拒绝编造"的场景

## 测试架构

### 双层测试模型
1. **工具层测试**：评估搜索工具返回的原始数据质量
2. **模型层测试**：评估使用搜索工具后生成的答案质量

### 评分方式
使用 AI Judge（当前 Claude 实例）根据预定义的判定标准自动评分。

---

## 📋 执行流程清单

当用户调用此 skill 时，按照以下清单逐项执行。**每完成一步，在思考过程中标记为已完成**。

### 🔷 阶段一：初始化准备

- [ ] **1.1 加载测试配置**
  ```
  Read: data/test-cases.yml
  解析：测试用例列表、测试目标、评分权重
  ```

- [ ] **1.2 环境检查**
  ```
  Read: modules/01-setup.md
  执行模块中的所有检查项：
    - [ ] 检查 Grok Search MCP 可用性
    - [ ] 创建测试会话目录
    - [ ] 初始化日志文件 (_test_log.jsonl)
    - [ ] 准备测试上下文对象
  ```

- [ ] **1.3 用户交互**
  ```
  AskUserQuestion 询问：
    问题 1: 选择测试维度（多选）
      ☐ 时效性 (3个用例)
      ☐ 准确度 (3个用例)
      ☐ 抗幻觉 (3个用例)
      ☐ 可核查性 (2个用例)
      ☐ 效率 (1个用例)
      ☐ 全部 (推荐)

    问题 2: 选择测试工具（多选）
      ☐ Grok Search MCP
      ☐ Built-in WebSearch
  ```

- [ ] **1.4 计算测试总数**
  ```
  total_tests = 选中的测试用例数 × 选中的工具数
  向用户显示：准备执行 {total_tests} 个测试
  ```

---

### 🔷 阶段二：测试执行循环

对每个 **(测试用例 × 工具)** 组合，执行以下子任务：

**当前测试**：[{test_case_id}] × [{tool_name}]

- [ ] **2.1 准备阶段**
  ```
  - [ ] 加载测试用例详细信息
  - [ ] 初始化计时器
  - [ ] 创建/打开该工具的答卷文件
  ```

- [ ] **2.2 执行搜索并生成答案**
  ```
  Read: modules/02-execution.md
  按照模块流程执行：
    - [ ] 构建搜索查询
    - [ ] 记录搜索开始时间戳
    - [ ] 调用搜索工具
    - [ ] 记录搜索完成时间戳
    - [ ] 记录原始搜索结果
    - [ ] 生成完整答案（自然语言）
    - [ ] 记录答案完成时间戳
    - [ ] 计算各阶段耗时
  ```

- [ ] **2.3 保存答案**
  ```
  - [ ] 将答案追加到答卷文件 {session_dir}/answers/{tool_id}.md
  - [ ] 包含：问题描述、原始搜索结果、Claude分析答案、时间戳
  - [ ] 追加索引到 _test_log.jsonl
  - [ ] 显示进度：✅ [X/total] {test_id} ({tool}): 答案已保存 | {time}秒
  ```

- [ ] **2.4 检查是否有更多测试**
  ```
  如果还有未完成的测试：
    → 返回 2.1，继续下一个测试
  如果当前工具的所有测试已完成：
    → 更新答卷文件的汇总统计
    → 继续下一个工具的测试
  如果所有测试已完成：
    → 进入阶段三（评分阶段）
  ```

---

### 🔷 阶段三：结果分析

所有答题完成后，对每个工具的答卷进行分析。

**对每个工具的答卷文件**：

- [ ] **3.1 读取答卷**
  ```
  Read: {session_dir}/answers/{tool_id}.md
  解析所有问题的答案、原始搜索结果和时间戳
  ```

- [ ] **3.2 逐题分析**
  ```
  Read: modules/03-judging.md

  对答卷中的每个问题执行分析：
    - [ ] 准备分析维度（基于判定标准）
    - [ ] 切换为分析者角色
    - [ ] 统计搜索结果数据
    - [ ] 分析结果有效性和问题
    - [ ] 生成分析报告（Markdown）
    - [ ] 显示进度：📊 分析中 [{tool}] {test_id}
  ```

- [ ] **3.3 保存分析结果**
  ```
  - [ ] 汇总所有问题的分析
  - [ ] Write: {session_dir}/analysis/{tool_id}_analysis.md
  - [ ] 更新 _test_log.jsonl
  - [ ] 显示完成：✅ [{tool}] 分析完成
  ```

---

### 🔷 阶段四：报告生成

- [ ] **4.1 读取测试数据**
  ```
  方式一：读取答卷文件和分析文件
    - Glob: {session_dir}/answers/*.md
    - Glob: {session_dir}/analysis/*_analysis.md
    - 解析答卷和分析数据

  方式二：读取索引文件（更快）
    - Read: {session_dir}/_test_log.jsonl
    - 根据索引读取对应的答卷和分析文件
  ```

- [ ] **4.2 生成报告**
  ```
  Read: modules/04-reporting.md
  按照模板生成：
    - [ ] 单工具详细报告（每个工具一份）
    - [ ] 多工具对比报告（如果测试了多个工具）
  ```

- [ ] **4.3 展示结果摘要**
  ```
  向用户显示：
    ✅ 测试完成！
    - 测试工具: {count}个
    - 测试用例: {count}个
    - 总耗时: {elapsed}秒

    📁 报告位置: {session_dir}/
      ├── answers/           # 答卷文件
      ├── analysis/          # 分析报告
      └── summary_comparison.md
  ```

---

### ✅ 完成标志

当以下所有条件满足时，skill 执行完成：
- ✅ 所有选中的测试用例已执行，答案已保存到答卷文件
- ✅ 所有答卷已完成分析，分析报告已生成
- ✅ 对比报告文件已生成
- ✅ 用户已收到结果摘要

---

## 🔧 实现原则

### 关键设计
1. **你是答题者，也是评判者**
   - 答题时：诚实使用搜索工具，给出最佳答案
   - 评判时：严格按判定标准，不偏袒任何工具

2. **答卷文件结构**
   - 每个工具生成一个答卷文件：`{session_dir}/answers/{tool_id}.md`
   - 答卷文件包含该工具的所有测试问题及答案
   - 每个问题包含三部分：
     * **原始搜索结果**：工具返回的完整 JSON 数据
     * **Claude 分析答案**：基于搜索结果生成的自然语言答案
     * **时间戳信息**：搜索开始/完成时间、答案生成完成时间
   - 分析时读取答卷文件，对每个问题进行分析
   - 分析结果保存到独立的分析报告：`{session_dir}/analysis/{tool_id}_analysis.md`

3. **严格遵循判定标准**
   - 不主观判断"答案好不好"
   - 只检查判定标准中列出的具体点
   - 参考评分校准示例：`modules/calibration.md`

4. **保持一致性**
   - 同一判定标准对所有工具采用相同尺度
   - 记录扣分理由，确保可追溯

### 错误处理
```
如果搜索工具调用失败:
  - 记录错误信息
  - 在报告中标注 ❌ 失败
  - 继续其他测试
```

---

## 📚 模块索引

当需要详细实现时，按需读取以下模块：

| 模块文件 | 用途 | 何时读取 |
|---------|------|---------|
| `modules/01-setup.md` | 环境检查和初始化 | 阶段一 |
| `modules/02-execution.md` | 测试执行详细流程 | 阶段二 - 执行测试 |
| `modules/03-judging.md` | 结果分析逻辑 | 阶段三 - 分析 |
| `modules/04-reporting.md` | 报告生成模板 | 阶段四 |

---

## 📄 答卷文件格式规范

每个工具生成一个答卷文件，包含该工具的所有测试问题及答案。

**文件路径**: `{session_dir}/answers/{tool_id}.md`

### 答卷文件结构

```markdown
# {Tool Name} 测试答卷

**生成时间**: {timestamp}
**工具标识**: {tool_id}
**测试用例数**: {total_cases}

---

## 问题 1: [{test_case_id}] {test_case_name}

**分类**: {category}
**问题描述**:
```
{test_case.prompt}
```

### 📥 原始搜索结果

**搜索查询**: {构建的查询字符串}
**结果数量**: {result_count}
**搜索开始时间**: {ISO8601}
**搜索完成时间**: {ISO8601}
**搜索耗时**: {X.XX}秒

<details>
<summary>完整 JSON 数据（点击展开）</summary>

\`\`\`json
{raw_search_results 的完整 JSON}
\`\`\`

</details>

### 💡 Claude 分析答案

**答案生成开始**: {ISO8601}
**答案生成完成**: {ISO8601}
**分析耗时**: {X.XX}秒

{生成的完整自然语言答案}

### ⏱️ 时间统计

| 阶段 | 耗时 |
|------|------|
| 搜索阶段 | {X.XX}秒 |
| 分析阶段 | {X.XX}秒 |
| **总耗时** | **{X.XX}秒** |

---

## 问题 2: [{test_case_id}] {test_case_name}

{重复上述格式}

---

... (所有测试问题)

---

## 📊 汇总统计

- **总测试数**: {count}
- **总耗时**: {X.XX}秒
- **平均耗时**: {X.XX}秒/题
- **最快响应**: {X.XX}秒 ({test_case_id})
- **最慢响应**: {X.XX}秒 ({test_case_id})
```

### 文件组织结构

完整的测试会话目录结构：

```
{session_dir}/
├── answers/                       # 答卷目录
│   ├── grok-search.md            # Grok Search 的答卷
│   └── builtin-websearch.md      # Built-in WebSearch 的答卷
├── analysis/                      # 分析报告目录
│   ├── grok-search_analysis.md   # Grok Search 的分析报告
│   └── builtin-websearch_analysis.md
├── _test_log.jsonl               # 测试日志（索引文件）
├── summary_comparison.md         # 多工具对比报告
├── grok-search_report.md         # 单工具详细报告
└── builtin-websearch_report.md
```

---

## 🚀 快速开始

首次运行建议：
1. 选择"全部"维度进行完整测试
2. 同时测试两个工具以获得对比数据
3. 完成后查看 `summary_comparison.md` 了解整体表现
