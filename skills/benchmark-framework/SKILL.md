---
name: benchmark-framework
description: 当用户询问"搜索工具测试"、"benchmark 评分标准"、"如何评估搜索质量"、"时效性测试"、"抗幻觉测试"或需要理解测试用例判定标准时，使用此技能
version: 2.0.0
invocable: false
---

# 搜索能力基准测试框架

## 概述

本框架提供系统化的搜索工具评估方法论，基于**问答式评估**而非传统的检索指标（如 Precision/Recall）。核心理念：

> **测试的不是"返回了什么数据"，而是"能否正确回答问题"**

## 五大评估维度

### 1. 时效性（Freshness）

**测试目标**：验证搜索工具获取和识别最新信息的能力

**核心判定点**：
- 是否混入超出时间窗口的信息？
- 是否明确标注统计区间？
- 信息不足时是否诚实说明而非编造？

**典型测试场景**：
- 硬时间窗口（如"过去 7 天"）
- 最近一次发布/更新
- 时间不确定场景

**答案标准**：
```
✅ 好答案：
"在过去7天内（2026-01-31 至 2026-02-07），找到2条更新：
1. [具体事件]（来源，2026-02-05）
2. [具体事件]（来源，2026-02-03）
注：7天内仅找到2条确切信息"

❌ 坏答案：
"AI领域的最新更新包括：..." (无明确时间范围)
```

---

### 2. 准确度（Factuality）

**测试目标**：验证对精确事实的核查能力

**核心判定点**：
- 年份、作者、数值是否准确？
- 是否引用官方来源而非第三方博客？
- 是否区分原始工作与后续改进？

**典型测试场景**：
- 精确事实核查（论文、发布时间）
- 参数/规格验证（配额、限制）
- 看似简单但容易错的事实

**答案标准**：
```
✅ 好答案：
"GitHub Actions 免费用户配额：
- 并发任务：20个
- 每月分钟数：2000分钟（Linux）
来源：https://docs.github.com/en/billing/..."

❌ 坏答案：
"GitHub Actions有一些限制..." (过于模糊)
```

---

### 3. 抗幻觉（Anti-hallucination）

**测试目标**：测试在信息不足或诱导性问题下的抗编造能力

**核心判定点**：
- 是否敢说"未找到证据/查无此事"？
- 是否识别部分真实+部分虚假的混合信息？
- 信息不足时是否先澄清再回答？

**典型测试场景**：
- 伪命题诱导（虚假发布信息）
- 部分真实+部分虚假
- 信息不足场景（模糊项目名）

**答案标准**：
```
✅ 好答案：
"经搜索，未找到 OpenAI 发布 GPT-5.5 的官方消息。
检查了 openai.com/blog 和官方 Twitter，
最新版本仍为 GPT-4 Turbo (2024-04-09)。"

❌ 坏答案：
"是的，GPT-5.5 已发布，主要特性包括..." (编造)
```

---

### 4. 可核查性（Verifiability）

**测试目标**：测试引用的规范性和可追溯性

**核心判定点**：
- 每个主张是否有独立来源？（禁止引用堆砌）
- 引用内容是否真的支持该主张？
- 争议性话题是否提供多方观点和来源？

**典型测试场景**：
- 主张-引用强绑定
- 争议性话题的双边引用

**答案标准**：
```
✅ 好答案：
"REST API 和 GraphQL 的核心差异：
1. 数据获取方式：REST多端点，GraphQL单端点
   来源：[GraphQL官方文档](https://graphql.org/learn/)
2. 过度获取：REST易过度，GraphQL按需
   来源：[Apollo教程](https://apollographql.com/...)
3. ..."

❌ 坏答案：
"差异包括：[列出3点]
参考资料：[链接1] [链接2]" (引用堆砌)
```

---

### 5. 效率（Efficiency）

**测试目标**：在保证质量前提下测试响应速度

**核心判定点**：
- 完整输出耗时（搜索+分析）
- 是否为省时间牺牲引用质量？
- 是否混入旧信息以加快速度？

**评判标准**：
- < 5 秒：优秀
- < 10 秒：良好
- > 15 秒：需改进

**注意**：效率不能以牺牲其他维度为代价

---

## 测试流程架构

### 阶段一：初始化准备
详见：`references/01-setup.md`

**关键步骤**：
1. 检查搜索工具可用性
2. 创建测试会话目录
3. 加载测试用例配置
4. 用户选择测试维度和工具

---

### 阶段二：测试执行
详见：`references/02-execution.md`

**关键步骤**：
1. 准备阶段：初始化结果对象，创建答卷文件
2. 执行搜索：构建查询，调用搜索工具，记录时间
3. 生成答案：基于搜索结果生成自然语言答案
4. 保存结果：追加到答卷文件和日志
5. 循环控制：自动进行下一个测试（**无中断**）

**执行原则**：
- ✅ 连续自动执行，不暂停询问用户
- ✅ 尽可能批量处理多个步骤
- ✅ 只显示进度信息，不请求交互

---

### 阶段三：结果分析
详见：`references/03-judging.md`

**关键步骤**：
1. 读取答卷文件
2. 对每个问题按判定标准逐项评分
3. 记录扣分理由
4. 生成分析报告

**评分原则**：
- ✅ 严格遵循 `judgment_criteria`
- ✅ 不主观判断"好不好"
- ✅ 参考 `references/calibration.md` 保持一致性
- ✅ 对所有工具采用相同尺度

---

### 阶段四：报告生成
详见：`references/04-reporting.md`

**输出文件**：
- `answers/{tool_id}.md` - 答卷文件
- `analysis/{tool_id}_analysis.md` - 分析报告
- `{tool_id}_report.md` - 单工具详细报告
- `summary_comparison.md` - 多工具对比报告
- `_test_log.jsonl` - 机器可读日志

---

## 答卷文件格式

每个工具生成一个答卷文件，包含：

### 文件头部
```markdown
# {Tool Name} 测试答卷

**生成时间**: {timestamp}
**工具标识**: {tool_id}
**测试用例数**: {total_cases}
```

### 每个问题的结构
```markdown
## 问题 {序号}: [{test_case_id}] {test_case_name}

**分类**: {category}
**问题描述**:
```
{test_case.prompt}
```

### 原始搜索结果
**搜索查询**: {query}
**结果数量**: {count}
**搜索开始时间**: {ISO8601}
**搜索完成时间**: {ISO8601}
**搜索耗时**: {X.XX}秒

<details>
<summary>完整 JSON 数据（点击展开）</summary>

```json
{raw_search_results}
```
</details>

### Claude 分析答案
**答案生成开始**: {ISO8601}
**答案生成完成**: {ISO8601}
**分析耗时**: {X.XX}秒

{生成的完整自然语言答案}

### 时间统计
| 阶段 | 耗时 |
|------|------|
| 搜索阶段 | {X.XX}秒 |
| 分析阶段 | {X.XX}秒 |
| **总耗时** | **{X.XX}秒** |

---
```

---

## 评分校准示例

详见：`references/calibration.md`

**核心校准原则**：
1. **一致性优先**：同一标准对所有工具用相同尺度
2. **证据驱动**：基于客观事实而非主观感受
3. **记录理由**：每次扣分必须说明原因
4. **参考示例**：不确定时查阅 calibration.md 中的评分案例

---

## 实现原则

### 关键设计

1. **双重角色**
   - 答题时：诚实使用搜索工具，给出最佳答案
   - 评判时：严格按判定标准，不偏袒任何工具

2. **自动化执行**
   - 所有测试连续自动完成，无需人工干预
   - 文件操作（保存答卷、日志）自动进行
   - 仅在开始前确认一次测试范围

3. **工具独立性**
   - 仅使用测试框架自身的工具和数据
   - 不依赖外部辅助工具

4. **进度透明**
   - 每完成一个测试立即显示进度
   - 测试完成后清晰展示报告路径

---

## 错误处理

```
如果搜索工具调用失败：
  - 记录错误信息到 test_result.error
  - 在报告中标注失败
  - 继续执行其他测试（不中断整个流程）
```

---

## 参考文档索引

| 文档 | 用途 | 何时查阅 |
|------|------|---------|
| `references/01-setup.md` | 初始化详细清单 | 阶段一 |
| `references/02-execution.md` | 单个测试执行流程 | 阶段二 |
| `references/03-judging.md` | 评分分析流程 | 阶段三 |
| `references/04-reporting.md` | 报告生成模板 | 阶段四 |
| `references/calibration.md` | 评分校准示例 | 评分时参考 |

---

## 快速参考

### 测试用例配置位置
`data/test-cases.yml`

### 会话目录格式
`/search-benchmark/{model-name}-{timestamp}/`

### 支持的搜索工具
- Grok Search MCP (`mcp__grok-search__web_search`)
- Built-in WebSearch (`WebSearch`)

### 测试用例总数
- 时效性：3 个
- 准确度：3 个
- 抗幻觉：3 个
- 可核查性：2 个
- 效率：1 个
**总计**：12 个测试用例
